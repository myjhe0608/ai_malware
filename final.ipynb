{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 41\n",
    "\n",
    "def read_label_csv(path):\n",
    "    label_table = dict()\n",
    "    with open(path, \"r\",encoding='ISO-8859-1') as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            fname, label = line.strip().split(\",\")\n",
    "            label_table[fname] = int(label)\n",
    "    return label_table\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_model(**kwargs):\n",
    "    if kwargs[\"model\"] == \"rf\":\n",
    "        return RandomForestClassifier(random_state=kwargs[\"random_state\"], n_jobs=4)\n",
    "    elif kwargs[\"model\"] == \"dt\":\n",
    "        return DecisionTreeClassifier(random_state=kwargs[\"random_state\"])\n",
    "    elif kwargs[\"model\"] == \"lgb\":\n",
    "        return LGBMClassifier(random_state=kwargs[\"random_state\"], n_estimators=300)\n",
    "    elif kwargs[\"model\"] == \"svm\":\n",
    "        return SVC(random_state=kwargs[\"random_state\"])\n",
    "    elif kwargs[\"model\"] == \"lr\":\n",
    "        return LogisticRegression(random_state=kwargs[\"random_state\"], n_jobs=-1)\n",
    "    elif kwargs[\"model\"] == \"knn\":\n",
    "        return KNeighborsClassifier(n_jobs=-1)\n",
    "    elif kwargs[\"model\"] == \"adaboost\":\n",
    "        return AdaBoostClassifier(random_state=kwargs[\"random_state\"])\n",
    "    elif kwargs[\"model\"] == \"mlp\":\n",
    "        return MLPClassifier(random_state=kwargs[\"random_state\"])\n",
    "    else:\n",
    "        print(\"Unsupported Algorithm\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def train(X_train, y_train, model):\n",
    "    clf = load_model(model=model, random_state=SEED)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def evaluate(X_test, y_test, model):\n",
    "    \n",
    "    predict = model.predict(X_test)\n",
    "    print(\"정확도\", model.score(X_test, y_test))\n",
    "\n",
    "def ensemble_result(X, y, models):\n",
    "    predicts = []\n",
    "    for i in range(len(X)):\n",
    "        probs = []\n",
    "        for model in models:\n",
    "            prob = model.predict_proba(X)[i][1]\n",
    "            probs.append(prob)\n",
    "        predict = 1 if np.mean(probs) >= 0.5 else 0\n",
    "        predicts.append(predict)\n",
    "        \n",
    "    print(\"정확도\", accuracy_score(y, predicts))\n",
    "\n",
    "def select_feature(x, y, model):\n",
    "    \n",
    "    model = load_model(model=model, random_state=SEED)\n",
    "    rfe = RFE(estimator=model)\n",
    "    return rfe.fit_transform(x, y)\n",
    "\n",
    "def process1(path_l,peminer,ember,pestudio) :   \n",
    "\n",
    "    V, w = [], []\n",
    "    x=0\n",
    "    label_table = read_label_csv(f\"{path_l}\")\n",
    "    a = os.listdir(f\"{peminer}\")\n",
    "    b = os.listdir(f\"{ember}\")\n",
    "    c = os.listdir(f\"{pestudio}\")\n",
    "    a.extend(b)\n",
    "    a.extend(c)\n",
    "    all_files = set(a)\n",
    "    for fname in tqdm(list(all_files)):\n",
    "        feature_vector = []\n",
    "        lname = fname.split('.')[0]\n",
    "        label = label_table[lname]\n",
    "        for data in [peminer,ember,pestudio]:\n",
    "            path = f\"{data}/{fname}\"\n",
    "            try :\n",
    "                if data == peminer :\n",
    "                    feature_vector += PeminerParser(path).process_report()\n",
    "                elif data == ember:\n",
    "                    feature_vector += EmberParser(path).process_report()\n",
    "                #else:\n",
    "                    #feature_vector += PestudioParser(path).process_report()   \n",
    "            except FileNotFoundError : pass\n",
    "        V.append(feature_vector)\n",
    "        w.append(label)\n",
    "    print(np.asarray(V).shape, np.asarray(w).shape)\n",
    "    return V, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeminerParser:\n",
    "    def __init__(self, path):\n",
    "        self.report = read_json(path)\n",
    "        self.vector = []\n",
    "    \n",
    "    def process_report(self):\n",
    "        self.vector = [value for _, value in sorted(self.report.items(), key=lambda x: x[0])]\n",
    "        return self.vector\n",
    "    \n",
    "\n",
    "class EmberParser:\n",
    "    def __init__(self, path):\n",
    "        self.report = read_json(path)\n",
    "        self.vector = []\n",
    "    \n",
    "    def get_histogram_info(self):\n",
    "        histogram = np.array(self.report[\"histogram\"])\n",
    "        total = histogram.sum()\n",
    "        vector = histogram / total\n",
    "        return vector.tolist()\n",
    "    \n",
    "    def get_string_info(self):\n",
    "        strings = self.report[\"strings\"]\n",
    "\n",
    "        hist_divisor = float(strings['printables']) if strings['printables'] > 0 else 1.0\n",
    "        vector = [\n",
    "            strings['numstrings'], \n",
    "            strings['avlength'], \n",
    "            strings['printables'],\n",
    "            strings['entropy'], \n",
    "            strings['paths'], \n",
    "            strings['urls'],\n",
    "            strings['registry'], \n",
    "            strings['MZ']\n",
    "        ]\n",
    "        vector += (np.asarray(strings['printabledist']) / hist_divisor).tolist()\n",
    "        return vector\n",
    "    \n",
    "    def get_general_file_info(self):\n",
    "        general = self.report[\"general\"]\n",
    "        vector = [\n",
    "            general['size'], general['vsize'], general['has_debug'], general['exports'], general['imports'],\n",
    "            general['has_relocations'], general['has_resources'], general['has_signature'], general['has_tls'],\n",
    "            general['symbols']\n",
    "        ]\n",
    "        return vector\n",
    "    \n",
    "    def get_byteentropy_info(self):\n",
    "        byteentropy = np.array(self.report[\"byteentropy\"])\n",
    "        total = byteentropy.sum()\n",
    "        vector = byteentropy / total\n",
    "        return vector.tolist()\n",
    "\n",
    "    def get_exports_info(self):\n",
    "        if len(self.report[\"exports\"]) > 0:\n",
    "            exports = self.report[\"exports\"]\n",
    "            vector = FeatureHasher(128, input_type = \"string\").transform(exports).toarray()[0]\n",
    "            return vector.tolist()\n",
    "        return [0]*128\n",
    "    \n",
    "    def get_header_info(self):\n",
    "        header = self.report[\"header\"]\n",
    "        if len(header[\"optional\"][\"dll_characteristics\"]) > 0:\n",
    "                dll_characteristics =FeatureHasher(10, input_type = \"string\").transform(header[\"optional\"][\"dll_characteristics\"]).toarray()[0].tolist()\n",
    "        else: dll_characteristics = [0]*10\n",
    "\n",
    "        if len(header[\"coff\"][\"characteristics\"]) > 0:\n",
    "                characteristics =FeatureHasher(10, input_type = \"string\").transform(header[\"coff\"][\"characteristics\"]).toarray()[0].tolist()\n",
    "        else: characteristics = [0]*10\n",
    "\n",
    "        if len(header[\"optional\"][\"magic\"]) > 0:\n",
    "                magic =FeatureHasher(10, input_type = \"string\").transform(header[\"optional\"][\"magic\"]).toarray()[0].tolist()\n",
    "        else: magic = [0]*10\n",
    "\n",
    "        if len(header[\"optional\"][\"subsystem\"]) > 0:\n",
    "                subsystem =FeatureHasher(10, input_type = \"string\").transform(header[\"optional\"][\"subsystem\"]).toarray()[0].tolist()\n",
    "        else: subsystem = [0]*10\n",
    "        \n",
    "        vector = []\n",
    "        vector += characteristics\n",
    "        vector += dll_characteristics\n",
    "        vector += magic\n",
    "        vector += subsystem\n",
    "\n",
    "        vector += [  \n",
    "                    header[\"optional\"][\"sizeof_code\"],\n",
    "                    header[\"optional\"][\"sizeof_headers\"],\n",
    "                    header[\"optional\"][\"sizeof_heap_commit\"]\n",
    "                ]\n",
    "        \n",
    "        return vector\n",
    "\n",
    "    def get_imports_info(self):\n",
    "        imports = self.report[\"imports\"]\n",
    "        dll = list(set([i.lower() for i in imports.keys()]))\n",
    "        hdll = FeatureHasher(256, input_type = \"string\").transform([dll]).toarray()[0]\n",
    "\n",
    "        funct = [i.lower() + ':' + k for i, j in imports.items() for k in j]\n",
    "        hfunct = FeatureHasher(1024, input_type=\"string\").transform([funct]).toarray()[0]\n",
    "\n",
    "        vector = []\n",
    "        vector += hdll.tolist()\n",
    "        vector += hfunct.tolist()\n",
    "\n",
    "        return vector\n",
    "        \n",
    "   \n",
    "    def process_report(self):\n",
    "        vector = []\n",
    "        vector += self.get_general_file_info()\n",
    "        vector += self.get_histogram_info()\n",
    "        vector += self.get_string_info()\n",
    "        vector += self.get_byteentropy_info()\n",
    "        vector += self.get_exports_info()\n",
    "        vector += self.get_header_info()\n",
    "        vector += self.get_imports_info()\n",
    "        return vector\n",
    "    \n",
    "class PestudioParser:\n",
    "    def __init__(self, path):\n",
    "        self.report = read_json(path)\n",
    "        self.vector = []\n",
    "    \n",
    "    def process_report(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:04<00:00, 309.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2265) (20000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:31<00:00, 314.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2265) (10000,)\n"
     ]
    }
   ],
   "source": [
    "train_path_l =   \"데이터/학습데이터_정답.csv\" #정답_데이터가 저장되어있는 위치의 경로 변수\n",
    "train_peminer =   \"데이터/PEMINER/학습데이터\"#peminer가 저장되어있는 위치의 경로 변수\n",
    "train_ember =     \"데이터/EMBER/학습데이터\"#ember가 저장되어있는 위치의 경로 변수\n",
    "train_pestudio =  \"데이터/PESTUDIO/학습데이터\"#pestudio가 저장되어있는 위치의 경로 변수\n",
    "vali_path_l =   \"데이터/검증데이터_정답.csv\" #정답_데이터가 저장되어있는 위치의 경로 변수\n",
    "vali_peminer =   \"데이터/PEMINER/검증데이터\"#peminer가 저장되어있는 위치의 경로 변수\n",
    "vali_ember =     \"데이터/EMBER/검증데이터\"#ember가 저장되어있는 위치의 경로 변수\n",
    "vali_pestudio =  \"데이터/PESTUDIO/검증데이터\"#pestudio가 저장되어있는 위치의 경로 변수\n",
    "\n",
    "train_V, train_w = process1(train_path_l, train_peminer, train_ember,train_pestudio)\n",
    "vali_V, vali_w = process1(vali_path_l, vali_peminer, vali_ember, vali_pestudio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 0.9455\n",
      "정확도 0.9571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = []\n",
    "for model in [\"rf\", \"lgb\"]:\n",
    "    clf = train(train_V, train_w, model)\n",
    "    models.append(clf)\n",
    "\n",
    "\n",
    "for model in models:\n",
    "     evaluate(vali_V, vali_w, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for model in [\"rf\", \"lgb\"]:\n",
    "    selected_X = select_feature(train_V, train_w, model)\n",
    "    new_model = train(selected_X, train_w, model)\n",
    "    models.append(new_model)\n",
    "\n",
    "ensemble_result(vali_V, vali_w, models)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
